---
title: Whale: Efficient Giant Model Training over Heterogeneous GPUs
date: 2024-05-06 16:25:00 +0800
categories: [TOP_CATEGORIE, SUB_CATEGORIE] #上级文文档，下级文档
tags: [TAG]     # TAG
math: true
---

## Abstract：
	扩大DNN已被证明能有效提高模型质量，但在训练效率、可编程性和资源适应性方面也面临着一些挑战。
	我们提出了巨型模型的通用高效分布式训练框架 Whale。
	为了支持各种并行策略及其混合策略，Whale 以模型注释的形式定义了两个新的原语，从而统一了编程接口，允许纳入用户提示。Whale  runtime利用这些注释并执行图优化，以转换本地深度学习 DAG 图，用于分布式多 GPU 执行。Whale 还引入了一种新颖的硬件感知并行策略，以平衡的方式提高了异构 GPU 上模型训练的性能。
	Whale 部署在一个拥有 512 个 GPU 的生产集群中，成功训练了一个拥有超过 10 万亿个模型参数的行业级多模态模型（命名为 M6），展示了极高的可扩展性和效率。

## Introduction
异构场景：在工业领域，数百个同构高端 GPU 的调度通常需要较长的排队时间。与此同时，获得异构 GPU（例如 P100 和 V100 的混合物）也更加容易。但使用异构 GPU 进行高效训练则更加困难，因为在建立模型时需要同时考虑 GPU 的计算单元和内存容量。此外，由于 GPU 的动态调度，用户在建立模型时并不知道硬件规格，这就造成了模型开发与硬件环境之间的差距。

我们提出了 Whale--一个专为训练巨型模型而设计的深度学习框架。与上述自动搜索高效模型分区或向用户暴露低级系统抽象和实现细节的方法不同，我们认为深度学习框架应适当提供高级抽象，以利用用户提示支持复杂的并行策略，尤其是在考虑使用异构 GPU 资源时。在这一原则的指导下，Whale 在 TensorFlow 的基础上扩展了两个必要的原语，从而实现了平衡。通过用这些原语注释本地 DL 模型，Whale 支持所有现有的并行策略及其组合，并通过自动重写深度学习执行图来实现。这种设计选择将并行策略与模型代码分离，并将其简化为数据流图，不仅减少了用户的工作量，还实现了图优化和资源感知优化，提高了效率和可扩展性。这样，Whale 就能让用户从模型训练的复杂执行细节中解脱出来，例如在多个设备上调度并行执行，以及在异构 GPU 之间平衡计算工作量。此外，Whale 还在生成分布式执行计划时引入了硬件感知负载均衡算法，从而在模型开发与异构运行环境之间架起了一座桥梁。

贡献：

	1.为了仔细平衡用户工作和分布式图形优化要求，Whale 引入了两个新的高级原语，用于表达所有现有的并行策略及其混合策略。
	2.通过利用注释进行图优化，Whale 可以将本地模型转化为分布式模型，并在多个 GPU 上高效、自动地对其进行训练。
	3.Whale 在训练具有十万亿个模型参数的最大多模态预训练模型 M6 方面树立了新的里程碑，证明了自己的能力。只需修改四行代码即可扩展模型，并在 512 个英伟达 V100M32 GPU 上运行。

## Background and Motivation

此外，不同的并行策略也适用于不同的模型分区。例如，大规模图像分类模型（即 10 万个类别）由图像特征提取分区和分类分区组成。图像特征提取分区需要对较少的模型参数进行大量计算。相反，分类分区包括低计算量的全连接层和软最大层，其模型大小往往是图像特征提取分区的 10 倍。因此，采用同构并行策略会影响这两个分区的性能。图 3 展示了一种更好的混合并行方法，即在特征提取分区中采用数据并行，在分类分区中采用张量模型并行，并将两者连接起来。![image-20240506151607481](C:\Users\fanji\AppData\Roaming\Typora\typora-user-images\image-20240506151607481.png)

GPU集群中的异构性：

   训练巨型模型相当耗费资源。此外，分布式模型训练通常要求资源同时到达。在工业界，用于巨型模型训练的共享集群通常与各种类型的 GPU（如 V100、P100 和 T4）混合使用，用于模型训练和推理。在异构 GPU 上训练巨型模型可降低同时收集所有所需 GPU（如数百或数千个 GPU）的难度，从而加快模型探索和实验的速度。然而，深度学习框架在有效利用异构资源方面遇到了挑战。不同类型的 GPU 在 GPU 内存容量（例如，P100 为 16GB，V100 为 32GB）和 GPU 计算能力方面各不相同，这自然会在计算图分割和深度学习算子分配方面带来不平衡。图 4 展示了在两个异构 GPU（即 V100 和 T4）上利用数据并行训练模型的情况。当平均分配训练样本时，V100 能更快地完成前向和反向的训练，从而在每个 mini-batch 结束时梯度同步之前留下空闲的 GPU 周期。通过动态生成执行计划时对硬件感知，Whale 为 V100 分配了更多的训练样本（即批量大小=4），为 T4 分配了其余 2 个样本，以消除空闲等待时间。
   结合先进的并行策略和异构 GPU 混合技术，不同的 GPU 内存容量和功能在对模型进行分区以实现有效重叠时需要进一步考虑，这是一个复杂的过程。模型开发人员在编程时很难考虑到所有资源问题，我们认为开发人员不应该这样做。对于通用深度学习框架来说，更好的方法是自适应地自动生成异构资源的执行计划。
   Whale 通过引入新的统一原语，用户可以专注于实现模型算法本身，同时只需更改注释即可在各种并行策略之间进行切换。Whale 运行时利用用户注释作为提示，在有限的搜索范围内通过自动图优化尽最大努力选择并行策略。Whale 还使用平衡算法进一步考虑了异构硬件能力，使资源异构性对用户透明。

## 设计

两个概念：
   任务图（TG）是并行转换和执行模型的子集。一个模型可以有一个或多个不重叠的任务图。我们可以对每个任务图应用并行策略。通过将模型操作模块化为任务图，Whale 可以对不同的模型部分应用不同的策略，也可以在流水线中调度任务图的执行。任务图可以进一步复制或分割。例如，在数据并行中，整个模型就是一个任务图，可以复制到多个设备上。在流水线并行中，一个流水线阶段就是一个 TaskGraph。在张量模型并行中，我们可以将 TaskGraph 分割成多个子模块来实现并行。
   虚拟设备（VD）是计算资源的逻辑表示，一个虚拟设备拥有一个或多个物理设备。VirtualDevice 向用户隐藏了设备拓扑、计算能力和设备放置的复杂性。一个虚拟设备分配给一个任务图。不同的虚拟设备可以拥有不同或相同的物理设备。例如，VD0 包含物理设备 GPU0 和 GPU1，VD1 包含物理设备 GPU2 和 GPU3（与 VD0 不同），VD2 包含物理设备 GPU0 和 GPU1（与 VD0 相同）。

并行原语：

   并行基元是一个 Python 上下文管理器，其中定义的操作被模块化为一个 TaskGraph。每个并行原语都必须配置一个参数 device_count，用于通过映射物理设备的 device_count 数量生成一个 VirtualDevice。Whale 允许用户使用两个统一的原语（即 replicate 和 split）来建议并行策略。这两个原语可以表达所有现有的并行策略，也可以表达它们的混合策略。
   replicate(device_count) 对要复制的任务图进行注解。device_count 是用于计算任务图副本的设备数量。如果没有设置 device_count，Whale 会为每个设备分配一个任务图副本。如果任务图被注释为 replicate(2)，它就会被复制到 2 个设备上，每个任务图副本消耗 mini-batch 的一半。因此，一个模型副本的 mini-batch 大小保持不变。
   split(device_count) 对任务图进行注解，以应用intra-tensor分片。device_count 表示要分片的分区数量。每个分片的分区都放在一个设备上。例如，split(2) 会将 TaskGraph 分割成 2 个分区，分别放在 2 个设备上。
   并行原语可以组合使用，对模型的不同分区应用不同的并行策略。此外，Whale 还提供了 JSON 配置 API 来实现系统优化。配置 auto_parallel 用于在给定分区编号 num_task_graph 的情况下启用自动任务图分区，这进一步简化了用户的编程，并且在资源分配是动态的情况下，对于硬件感知优化是必要的（第 3.3 节）。在 Whale 中，流水线并行被视为一种高效的任务图间执行策略。Whale使用配置 num_micro_batch，当其值大于 1 时，可在任务图之间实现高效的流水线并行。通过这种方式，Whale 将任务图的生成与流水线并行策略的选择分离开来。该系统可以轻松扩展，纳入更多流水线策略（例如，交换图 1 中 M1 的 B0 和 F1 的执行顺序）。
   除了并行策略或流水线并行的组合，Whale 还进一步支持整个并行模型的嵌套数据并行。当可用设备数量是任务图要求的设备总数的倍时，嵌套数据并行就会自动启用。

![image-20240506155323045](C:\Users\fanji\AppData\Roaming\Typora\typora-user-images\image-20240506155323045.png)

   例 1 显示了两个任务图的流水线并行示例，每个任务图配置 1 个设备。通过将 pipeline.num_micro_batch 配置为 8 启用了流水线并行。如果可用设备数为 8，即总设备数的 4 倍，Whale 将在流水线之外应用嵌套的 4 度数据并行。相反，当使用两个可用设备时，则是纯流水线。示例 2 展示了一种混合策略，即在复制 ResNet50 特征部分的同时，拆分图 3 示例中的分类模型部分。

![image-20240506155520062](C:\Users\fanji\AppData\Roaming\Typora\typora-user-images\image-20240506155520062.png)

   示例 3 展示了一个包含两个任务图的自动流水线示例。启用 auto_parallel 后，Whale 会根据计算资源容量和模型结构自动将模型划分为任务图。(第 3.3 节）

并行规划器负责生成高效的并行执行计划，这是 Whale runtime的核心。

![image-20240506155909963](C:\Users\fanji\AppData\Roaming\Typora\typora-user-images\image-20240506155909963.png)

图 5 显示了并行规划器的概览。工作流程可描述如下：(a) 并行规划器采用带有可选用户注释的本地模型、计算资源以及其它输入的配置信息。模型超参数（如批量大小和学习率）和计算资源（如 GPU 和 Worker）由用户手动决定。而并行原始注释和配置（如 num_task_graph 和 num_micro_batch）既可以由用户手动决定，也可以由 Whale 自动决定；(b) 根据计算资源和可选注释自动生成虚拟设备（第 3.2.1 节）；和 (c) 将模型划分为任务图（TaskGraph），如果对拆分进行了注释，则在内部对任务图进行进一步划分。由于我们允许对不同的任务图采用不同的策略，因此任务图之间可能存在输入/输出不匹配的情况。在这种情况下，规划器会在两个任务图之间自动插入相应的桥接层（第 3.2.3 节）。

3.2.1 虚拟设备生成

虚拟设备是根据每个任务图所需的设备数量生成的。给定 K 个物理设备 GPU0、GPU1、......、GPUK 和一个包含 N 个任务图的模型，对应的设备编号为 d1、d2、......dN。对于第 i 个任务图，Whale 将生成一个虚拟设备（VirtualDevice），其物理设备数量为 di。

3.2.2 任务图划分

Whale 首先通过显式注释或系统自动分区将模型划分为任务图（TaskGraph）。如果给出了用户注释，那么在某些并行基元注释中定义的操作就会组成一个任务图。否则，系统将根据给定的配置参数 num_task_graph 和硬件信息生成任务图。硬件感知模型分区的详情将在第 3.3 节中介绍。

3.2.3 Bridge layer

当对不同的任务图采用不同的并行策略时，输入/输出张量的数量和形状可能会因并行程度不同或并行策略不同而发生变化，从而导致任务图之间输入/输出张量形状的不匹配。为了解决这种不匹配问题，Whale 提出了一个桥层来收集分布式张量，并将其输入到下一个任务图中。


## Whale的硬件感知并行策略

