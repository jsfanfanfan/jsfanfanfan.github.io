---
title: "A Codesign of Scheduling and Parallelization for Large Model Training in Heterogeneous Clusters"
date: 2024-05-18 17:00:00 +0800
categories: [Literature, SUB_CATEGORIE] #上级文档，下级文档
tags: [Heterogeneous, Sparse]     # TAG
math: true
---

# A Codesign of Scheduling and Parallelization for Large Model Training in Heterogeneous Clusters

## Abstract

联合考虑 __调度__ 和 __自适应并行__ 为提高异构 GPU 集群中大模型的训练效率提供了巨大的机会。然而，将自适应并行集成到集群调度器中扩大了集群调度空间。新的空间是原始调度空间和自适应并行的探索空间（PP、DP 和 TP 并行）的乘积。指数级扩大的调度空间和自适应并行不断变化的最优并行调度共同导致高效集群调度的低开销和准确的性能数据获取之间的矛盾。

本文提出了 Crius，这是一种训练系统，用于在异构集群中通过自适应并行有效调度多个大模型。 Crius 提出了一种新颖的调度粒度，称为 Cell。它代表一个具有确定资源和流水级的作业。 Cell 的搜索空间缩小为仅 DP 和 TP 的乘积，从而展现出准确且低开销的性能估计的潜力。然后 Crius 准确地估计 Cells 并有效地安排训练作业。当选择一个 Cell 作为调度选项时，其代表的作业将按照探索的最佳并行策略运行。

## 1 Introduction

虽然训练大模型需要大量并行资源，但集群不断集成新一代 GPU 以满足高计算需求。这会导致 GPU 类型、GPU 互连拓扑等方面的异构性。为了处理异构性，__自适应并行__ 通过结合 DP, PP, TP 自动探索训练作业在分配资源上的最佳并行性。通过穷举搜索，自适应并行始终可以充分利用分配的资源。

然而，当前的集群调度器在将异构资源分配给训练作业时没有考虑自适应并行。__如果我们能够共同考虑调度选择和自适应并行性，我们就会找到提高整个训练集群效率的新机会__。

<img src="../assets/img/CoDesign/fig1.png" align="center" alt="figure 1">

如图 1 所示，调度程序通过在作业之间 scaling（case A）和/或 exchange（case B）资源并以自适应并行运行它们来实现更高的集群吞吐量。效率的提高还意味着有可能为个人用户实现更好的调度决策，即更短的排队时间或JCT。

一般来说，当调度器拥有整个调度空间中所有调度 candidates 的精确性能数据时，它可以做出有效的决策。遵循这一思想，当前集群调度的工作主要基于大量性能 profiling 来提高调度效率。它们无法与自适应并行集成以有效利用上述机会，因为在对所有可能的资源进行 profiling，自动探索会带来巨大的开销。具体来说，__新的调度空间是原始调度空间与自适应并行性的并行性探索空间的乘积。指数级扩大的调度空间阻碍了性能数据的获取__。

在不考虑自适应并行性的情况下，某些调度工作通过简单扩展来估计作业在未知资源上的性能。虽然此类解决方案的 profiling 开销较低，但它们是针对仅通过 DP 扩展的作业量身定制的。如果涉及自适应并行，由于最佳并行策略与假设并行策略之间的不匹配，他们的估计无法保持高精度。当然，如果我们能够在不进行探索的情况下预测自适应并行性的最佳策略，那么分析开销也可以减少。然而，我们的实验证明，由于探索空间较大（§2.3），预测精度无法保证。最终，低准确度导致调度效率低下。

因此，本文提出了 Crius，这是一种全面的训练系统，可以在异构集群中通过自适应并行有效地调度多个大模型。 Crius 通过一个名为 Cell 的新颖抽象来实现其目标。 Crius 将整个调度空间跨资源分配和 PP 维度划分为 Cell，每个 Cell 代表一个具有指定资源分配和流水级的作业。在调度训练作业时，Crius 会评估作业在不同 Cell 上的表现，并为它们选择最好的一个。一旦安排了 Cell，Crius 就会自动搜索其最佳并行策略并运行作业。通过将确定流水级的步骤移至调度空间，在运行时为作业寻找最佳并行计划的探索空间缩小了。

Cell 的关键见解是，具有自适应并行的调度遵循 cluster-friendly workflow ：__每个调度选择首先分配资源，然后确定流水级，最后将每个流水级独立地划分为数据和张量并行的组合__。虽然在分配资源之前确定并行性似乎也是可行的，但它可能会导致训练吞吐量次优。这是因为所使用的并行性的资源需求与集群的最佳调度决策分配的资源之间可能不匹配。改变工作流程会降低调度效率。

要利用 Cell，快速估计 Cell 的性能至关重要，而一项作业可能有多个 Cell。 Crius 建议离线分析所有通信 operators ，并在 runtime 使用仅数据并行和仅张量并行计划独立地分析模型流水级的计算。它是基于我们的观察而设计的，即作业的通信部分在调度之前是已知的，但计算部分需要在调度之后通过编译技术进行优化[25, 26]。runtime profiling 是在单个 GPU 上完成的，与分布式编译等价，硬件要求较低。然后，Crius 通过组合所有阶段的 2 个profiled 并行调度来组装 $2^{N_S}$ 并行计划（ $N_S$ 表示流水级编号）并在流水级之间注入预先配置的通信运算符。 Crius 使用组合计划中性能最好的一项作为 Cell 的性能。用于估计的计划不是最优的，但对于高效调度来说它具有足够的代表性，因为并行策略集成是跨 Cell 整个探索空间的基于网格的采样。

选择一个Cell后，Crius进一步探索最佳的并行方案。这种探索也可能非常耗时，因为调度算法可能会迁移和重新调度训练作业，并且在此过程中会一遍又一遍地触发探索。 Crius 利用 Cell 的组合计划来指导和剪枝探索，因为它们本质上反映了 Cell 的并行性偏好。

本文做出以下贡献：

• 我们发现在现有训练集群中应用自适应并行性的主要障碍是在如此大的调度空间中高效调度和低开销性能分析之间的矛盾。

• 我们提出了一个优雅的抽象，表示为Cell，作为适当的调度候选者，促进快速、准确的性能估计。

• 我们引入了一种全新的 estimator，能够以适度的硬件和时间开销准确估计 Cell 的性能，从而提高调度效率。

## 2 Background and Motivation

### 2.1 Training with Adaptive Parallelism

自适应并行[8-10]可以实现自动并行优化，无需手动设计。它通过构建并行计划空间并自动探索以找到最佳计划来实现这一目标。

<img src="../assets/img/CoDesign/fig2.png" align="center" alt="figure 2">

图 2 显示了自适应并行的示例。它通过在相应的资源上运行来枚举所有并行计划以获得实际性能。然后选择其中性能最好的一个作为最终的并行计划，以有效地训练预定的大型模型。通过以上过程，自适应并行性以硬件资源和调优时间为代价提高了作业的训练效率（在最先进的系统中进行一次探索需要 40 分钟 [8]）。

### 2.2 Scheduling Opportunities
通过自适应并行，作业始终可以充分利用分配的资源和足够的内存。然而，它仅从单个作业的角度来看表现最佳。如果集群调度器可以调整训练作业之间的资源并为它们搜索最佳并行计划，则可以提高集群吞吐量。资源扩展和自适应并行的结合考虑极大地扩大了调度空间，从而在集群级别带来新的调度机会。

我们进行了两项实验来检验新的机会。涉及三个大型模型：WRes、BERT 和 MOE。 GPU 通过节点内的 NVLink 和节点之间的 Infiniband 连接。更多详细信息如表 2 所示。图 3 描述了使用自适应并行调度这些模型时不同选择的吞吐量。我们使用所有训练作业吞吐量的总和作为集群吞吐量。

<img src="../assets/img/CoDesign/fig3.png" align="center" alt="figure 3">

图 3(a) 显示了在四个排队训练作业之间扩展同构资源的示例。我们观察到不同扩展方案的总体吞吐量差异很大。主要原因是使用相同资源的作业贡献不同的吞吐量。例如，WRes-2B 占用大量资源（8GPU），但贡献的吞吐量很少。集群调度程序有机会缩小此类模型的 GPU 数量，从而允许运行更多作业并提高集群吞吐量。

我们还发现了通过在排队作业之间交换异构资源来提高集群吞吐量的机会。例如，图 3(b) 展示了在两种类型的硬件上部署两个模型时的两种可用共享方案。如图所示，两种调度之间地总吞吐量达到61.9%。这主要是因为当BERT-2.6B从具有4个A100 GPU的服务器调度到具有4个V100 GPU的服务器时，由于内存限制，它必须使用张量并行。具有张量并行的 BERT-2.6B 性能急剧下降。

反过来，提高集群吞吐量有可能显着优化作业排队时间、作业完成时间等。此外，当模型变得更大并且硬件表现出更多异构性时，自适应并行的探索空间就会增加。资源扩展将为集群带来更多机会。

### 2.3 Contradiction in Efficient Scheduling
在§2.2中，高效的调度决策是基于每个模型在特定资源上运行时的最佳并行性的准确性能数据得出的。为了获取此类数据，基于 profiling 的工作需要使用自适应并行在所有可分配资源上分析每个训练作业。然而，自适应并行通过大量的作业扩展选择和并行探索极大地扩大了调度空间。考虑到第 2.1 节中的调整开销，这种 full-space profiling 是不可接受的。

同时，如果模型在具有自适应并行的特定资源上运行时能够准确估计模型的性能，我们还可以实现高效的调度。然而，当前基于估计的工作仅关注数据并行工作。

<img src="../assets/img/CoDesign/fig4.png" align="center" alt="figure 4">

图 4 中的实验结果表明，通过自适应并行获得的模型最优并行计划是不断变化的，并且作业性能变化差异很大。在这种情况下，这些工作中采用的简单缩放无法获得准确的估计结果。而且确定一个直接预测最优并行配置的总方法是很难的。

低开销和准确的性能估计之间的矛盾阻碍了具有自适应并行性的大型模型的高效调度。上述分析激发了Crius的设计，将自适应并行扩大的调度空间分割成适当的粒度作为调度候选。新的调度候选可以让调度器以低成本和高精度获取性能数据，从而提供高效的调度。

## 3 Crius Design

Crius的核心是Cell，它作为新的调度候选者。 Cell 定义了训练作业的资源和流水级确定的并行探索空间。它保证了异构集群中准确且低开销的性能数据采集。 Cell通过以下方式统一集群调度和模型并行化：__在调度时，集群调度器估计所有Cell的性能并对其进行高效调度；调度后，自动探索Cell的并行计划__。

<img src="../assets/img/CoDesign/fig5.png" align="center" alt="figure 5">

图 5 展示了 Crius 的概述。目前，它要求模型开发人员指定提交的训练作业所需的初始 GPU 数量。基于给定GPU数量和集群状态，Crius 为每个排队作业生成多个 Cell 作为集群的调度候选者（§6）。每个 Cell 流水级由指定的资源和作业的模型结构共同确定（第 6.1 节）。对于每个 Cell，Crius 的 agile estimator 提供准确的性能估算。该估计器采用计算通信解耦方法，带来较低的时间和硬件开销（§5.1）。根据估计结果，Crius 针对不同的调度目标对 Cell 进行整体调度（§6）。此外，为了支持自适应并行和作业重新调度的联合使用，并且开销小到忽略不计，Crius 提出了一种 Cell-limited 并行调优器。调优器通过估计结果剪枝探索空间来实现目标（第 5.2 节）。

## 4 Cell as the Core Abstraction

在本节中，我们首先研究在集群调度中集成自适应并行的工作效率。基于该性质，我们引入了Crius的核心抽象Cell，用于对调度空间进行划分。

### 4.1 Cluster-friendly Scheduling Workflow

在通用调度器中，调度空间由两个维度组成：选择调度作业和为其分配资源。

<img src="../assets/img/CoDesign/fig6.png" align="center" alt="figure 6">

如图 6 所示，它们分别表示在外部垂直轴和水平轴上。将自适应并行性\融入集群调度后，新的调度空间是原始调度空间与并行性探索空间的乘积空间，由 PP、DP 和 TP 三个维度组成。

虽然并行探索扩大了调度空间，但生成每个并行计划遵循典型的工作流程，如图 2 右侧所示。首先，由于并行计划将大模型划分到分配的 GPU 上，因此并行计划的生成高度依赖于分配的资源。同时，并行计划中包含的三个并行之间存在隐式优先级。PP 将整个模型切分为多个阶段，每个阶段都是一个子模型，可以通过 DP 或 TP 进一步独立划分[8]。因此，PP 需要优先于DP 和 TP。DP 和 TP 是每个流水级的内部并行，具有相同的优先级。

当考虑训练集群调度器中的自适应并行时，上述工作流程恰好涵盖了调度决策过程。改变工作流程会降低集群调度程序的效率。例如，如果模型的 PP 度确定为4，则调度器必须分配至少4个GPU才能使模型正常运行。这意味着调度程序失去了将作业资源扩展到更少 GPU 的可能性，而这可能是集群的最佳调度决策。这意味着 Crius 在将自适应并行性集成到集群调度中时必须遵循 cluster-friendly scheduling 的工作流程。

### 4.2 Sharding Scheduling Space into Cells

__Cell__。考虑到 cluster-friendly workflow ，Crius 提供了适当粒度的 Cell 来对调度空间进行分片。如图6所示，自适应并行扩大的调度空间被分片为Cell。每个 Cell 代表一个具有确定的资源分配和流水级的作业。在Cell内部，DP 和 PP 还有待自适应并行的探索。

通过 Cells，集群调度器可以通过寻找合适的 Cells 来调度训练作业。调度 Cell 后，可以使用自适应并行进一步探索其内部并行性。这样，不仅Cell可以缩小搜索空间来实现准确、低开销的性能数据采集，而且Crius可以利用这些性能数据进行高效的调度。而且，Crius 仍然遵循 cluster-friendly 工作流程。

__Cell流水级的确定__。由于 PP 已从并行探索中剥离出来，Crius 需要在集群调度程序级别确定 Cell 的流水级。为了实现流水线的高效率，Crius 通过遵守公认的原则来划分模型，即保持每个阶段的计算延迟相似并最大限度地减少阶段之间的通信。一般来说，Crius 根据浮点运算 (FLOP) 将分配的 GPU 映射到模型的各个算子，然后将它们聚集到具有最小化通信的流水级。

<img src="../assets/img/CoDesign/fig7.png" align="center" alt="figure 7">

图 7 展示了 Crius 详细阶段划分的示例。假设分区模型有 6 个 operators，分配了 8 个 GPU，我们首先根据每个operators 的 FLOP 与整个模型的比率，将 8 个 GPU 映射到每个 operator。例如，由于 OP1 占整个模型的 1/16 FLOP，而 OP2 占 3/16，因此 OP1 获得 0.5 个 GPU，OP2 获得 1.5 个 GPU。同时，每个算子的执行时间理论上可以计算为 $T_{elapsed} = FLOPs / Number_{GPU}$ 。这意味着在上述资源分配方案下，所有 operators 的执行时间是相同的。这样，即使每个 operator 都是一个独立的流水级，理论上 operators 也可以构建一个 full-state 流水线。

通过上述映射，Crius可以将 operators 聚类到各个流水级，同时保证所有流水级执行时间的相似性。图 7 显示了一个示例，operator clustering 选择最小的 3 个 inter-operator 通信作为聚类边界，将整个模型分为 4 个流水级。流水级确定完成后，算子映射的 GPU 会累积为该流水级分配的 GPU。请注意，每个阶段累积的 GPU 数量近似于 2 的幂。这是训练集群中常见的 GPU 拓扑。

__划分后的复杂度分析__。 Cell 抽象通过将 PP 与原始自适应并行解除绑定来减少并行搜索空间的大小。因此，由于 PP ，调度复杂度增加。幸运的是，延长的调度时间在训练集群中并不是一个大问题，因为训练作业通常需要很长时间（超过几个小时）。我们还在第 8.7 节中报告了调度开销。

## 5 Mechanisms for Leveraging Cell

Cell 统一了集群调度器和训练框架。在集群级别，调度器依赖于 Cell 的性能估计来进行高效调度。在框架层面，训练作业需要Cell定义的新搜索空间的最优并行计划，以充分利用分配的资源。因此，在本节中，Crius主要提出了两种机制：为调度器敏捷地估计Cell（§5.1）和在Cell的框架限制下调整并行性（§5.2）。

### 5.1 Agile Cell Estimation

__解耦通信和计算__。为了实现准确、快速的估计，我们研究了大模型的训练。

<img src="../assets/img/CoDesign/fig8.png" align="center" alt="figure 8">

图 8 说明了使用最佳并行计划训练大模型的示例。大模型由 4 个流水级组成，运行在总共 8 个 GPU 上，如图所示。DP 或 TP 将每个流水级进一步划分到 2 个 GPU 上，以充分利用 GPU 的计算能力。 send/recv 和 all_gather 等通信运算符连接各个流水级以形成整个训练流水线。

显然，训练执行由两类操作组成：计算和通信。在优化方面它们有很大的不同。至于通信，优化算法取决于 GPU 和节点之间的互连。由于硬件设置后互连几乎没有变化，因此通信操作的延迟性能仅因传输数据量而变化。至于计算，根据运行时计算运算符的特定模式应用各种编译优化。

优化计算和通信的独立性促使 Crius 将端到端性能估计解耦为两部分：计算部分和通信部分。离线时，Crius可以离线获取通信操作的分布式相关性能数据。在运行时，Crius 仅分析计算操作。在这种情况下，我们可以通过积累计算和通信操作的 profiling 数据来减少性能数据获取的开销。

由于解耦，Crius开发了一种快速的Cell估计方法。尽管Cell的流水级已经确定，但其并行探索空间中仍有许多选择。因此，该方法以 grid-based 的方法对Cell的搜索空间进行采样，并使用采样计划中最好的作为 Cell 的估计。这样，Crius 通过代表性采样来提供 Cell 的准确性能数据，而不是预测最优计划。

__通过并行组装进行采样__。粗略采样可能会在时间和硬件方面产生很高的开销。由于端到端计算可以解耦，我们发现我们可以组装流水级来生成不同的并行计划以减少开销。如图8所示， send/recv 用于连接按 DP 划分的流水级，all_gather 用于连接按 TP 划分的流水级。对于由 DP 和 TP 混合划分的阶段，需要协同使用 send/recv 和 all_gather。这意味着可以通过更改已知并行计划的阶段并行性和关联的通信运算符来组装新的并行计划。

因此，Crius 使用并行组装对 Cell 的探索空间进行采样，如图 9 所示。

<img src="../assets/img/CoDesign/fig9.png" align="center" alt="figure 9">

假设Crius 使用仅 TP 和仅 DP 预先了解每个流水级计算的性能数据，并且能够预测相关通信操作的性能，Crius 通过以下方式组装新的并行计划。它结合了每个流水级的两个已知并行计划，并注入通信操作来组装 =$2^{N_S}$ 不同的并行计划。采样过程是明确网格化的，并且可以以两次物理分析操作为代价来进行。

__单设备分布式分析__。为了估计采样的并行计划，Crius 进一步提出了单设备分布式分析，以获取指定并行计划和资源下作业的性能。分析也基于解耦观察。我们在图 10 中展示了单设备分布式分析的工作流程。

<img src="../assets/img/CoDesign/fig10.png" align="center" alt="figure 10">

在分析指定的并行计划（仅 DP 或仅 TP）时，Crius 从训练流水线中提取计算和通信操作。然后，Crius 对计算操作执行分布式等效编译并对其进行 profiling。这完全是在单个 GPU 上完成的。但对于通信操作来说，Crius 通过基于流量的插值来估计它们。用于插值的数据是离线获得的。

我们在Alpa [8]中采用了类似的方法来累积端到端延迟的计算和通信操作。如图10右下所示，整个训练流水线的延迟由两部分决定：
(1) 第一个微批次通过流水线的延迟 (T1 + T2 + T3 + T4)，(2) 由最慢流水级主导的其余 B − 1 个微批次的执行时间 ((B − 1) * (T3 -Tcomm.))。请注意，每个阶段的延迟是其计算运算符的延迟以及它与前一阶段之间的通信运算符的延迟之和。但在计算第二部分时，通信与计算重叠，因此需要减去。

基于上述方法，Crius可以获得所有组装的并行计划的端到端延迟。其中最好的作为 Cell 的估计。虽然该方法不能直接预测最优计划，但基于网格的采样覆盖了整个探索空间。表现最佳的抽样计划足以实现高效调度。

为了避免 OOM，Crius 还记录了每个阶段计算算子的内存使用情况。如果内存使用量超过所用GPU的内存限制，则该阶段相应的并行计划将被删除。

### 5.2 Cell-guided Parallelism Tuning

当选择一个Cell作为最佳调度选择时，相应的作业需要一个并行计划来最好地利用分配的资源。尽管快速估计足以实现高效调度，但使用估计选择的并行计划可能无法提供最佳性能。这就需要 Crius 探索Cell定义的空间来寻找最优的并行计划。不幸的是，之前的工作中的并行性探索需要枚举并行性计划，这会产生很高的开销。因此，Crius需要设计一种快速的并行调优方法。

在Crius中，我们利用Cell的估计结果来指导和加速Cell内部的调整。我们注意到，Cell 用于估计的并行计划正在接近最优计划。因此，Crius 根据估计来修剪探索空间。

<img src="../assets/img/CoDesign/fig11.png" align="center" alt="figure 11">

图 11 显示了空间剪枝的示例。在估计计划中选择的每个阶段的并行度被视为该阶段的并行度偏好。 Crius将单个阶段的完整探索空间分为两部分：（仅限数据并行到半混合并行）和（半混合并行到仅张量并行）。如果选择数据并行度作为某个阶段的首选并行度，则该阶段将仅在（数据并行度至半并行度）范围内进行调整，另一半将被修剪。这同样适用于张量并行性。

## 6 Cluster Scheduling Scheme with Cells

本节我们介绍Crius如何基于Cell的估计来高效调度大模型。

__Crius 中的调度__。在讨论调度细节之前，我们想强调一下 Crius 关键设计的目标。 Crius 旨在通过自适应并行为高效调度提供准确的性能数据。因此，Crius 目前采用一种简单的启发式算法来贪心地调度具有资源扩展性的大模型训练作业。基于求解器 [20,21,33] 的技术也可以用于增强 Crius，这与 Crius 的主要关注点正交。此外，虽然 Crius 的目标是在本节中最大化集群吞吐量，但 Crius 很容易适应其他调度目标。在§8.5中，我们扩展了 Crius 以支持截止时间感知的目标，并通过综合评估讨论其性能。

### 6.1 Scheduling Policy

<img src="../assets/img/CoDesign/ag1.png" align="center" alt="algorithm 1">

__策略__。算法1显示了Crius调度的伪代码。基本上，当新作业到达（第 2-8 行）或作业完成（第 9-13 行）时，就会触发该算法。当新作业到达时，算法初始化 Cells（第 4 行）并根据 Cells 进行调度（第 6 行）。特别的，调度算法（第 14-20 行）探讨了资源扩展的潜在调度选择。通过对 Cell 的性能估计，算法选择最佳选择（例如最大集群吞吐量）并重新调度所有作业。如果找不到可行的调度选择，则作业处于待处理状态。当正在运行的作业完成时，算法首先尝试恢复挂起的作业。如果没有挂起的作业，算法会触发额外的调度，将释放的资源分配给正在运行的作业（第 11 行）。请注意，在每个基于单元的调度中，最佳计划实际上应用于正在运行的作业。真正的作业调度只发生在最后（第 8 行和第 13 行）。

__初始Cells__。对于即将到来的新工作，Crius 需要为其初始化 Cells。 Crius 现在要求用户为提交的作业指定初始 GPU 编号 NG。根据 GPU 编号，Crius 选择 3 个可用值 $（N_G/2、N_G、2N_G）$ 来生成 Cell。至于流水级数，Crius 允许 $\log N_G$ 选择范围为 1 到 $N_G$。由于 Crius 需要单个设备来分析每种 GPU 上的作业，因此异构 GPU 上的分析是并行完成的。因此，为新作业生成 Cell 的总时间复杂度为 $O(3 * \log N_G)$。我们在第 8.2 节中评估了总体分析开销。

__扩大训练任务__。在调度新作业时，Crius 探索了资源扩展的调度选择。此探索尝试扩展正在运行的作业和新加入的作业的 GPU 类型和数量。当集群资源不足时，Crius会选择作业缩减其使用的GPU数量或将其移动到资源充足的另一种类型的GPU。当存在空闲资源时，Crius 执行反向扩展。扩展原则是在集群资源约束下搜索所有的调度选择。具体来说，一次调度选择中所有Cell的资源总和不应超过集群资源限制。为了避免调度开销，Crius 使用了一个称为搜索深度的超参数，它是算法 1 第 18 行中生成调度选择的最大作业扩展时间。我们在第 8.7 节中评估了搜索深度在开销和调度效率方面的影响。

__机会主义的执行__。为了避免需要大量 GPU 的模型出现饥饿，Crius 采用了机会主义执行。当空闲资源不足以用于排队作业时，Crius 会挂起该作业并伺机启动其他作业来利用空闲资源。一旦满足待处理作业的最低资源要求，机会性作业将被暂停以启动待处理作业。

## 7 Implementation
我们使用 10,900 LoC 的 Python 实现 Crius：5,400 行用于调度程序，5,100 行用于 Cell 的敏捷估计器，400 行用于 Cell 引导的并行调优器，而 Crius 模拟器仅 400 行。具体来说，我们使用 gRPC 在调度程序和分布式工作人员之间进行通信。在运行时，Crius 以 5 分钟的间隔调度作业，并每 20 秒检查一次训练作业的运行状态。

我们基于 Alpa [8] 实现了 Crius 的并行调优器。在这种情况下，Crius 的估计器是基于 Jax [34] 和 XLA [35] 构建的，这也是 Alpa 的后端。 Crius 在单个设备上模拟分布式训练，以获得所有管道阶段的分布式等效编译。此外，估计器有一个大约 500 LoC 的 C++ 后端，使 Crius 能够利用 Nvidia CUPTI [36] 捕获计算运算符的延迟。基于这些技术，Crius 的估计器以较低的开销获得了计算算子的准确延迟。对于通信运营商来说，离线分析是基于 XLA [35]、NCCL [37] 和 Ray [38] 对所有使用的 GPU 实现的。

## 8 Evaluation
在本节中，我们评估 Crius 在异构物理集群和具有三个生产轨迹的模拟大型集群上的有效性。

### 8.1 Experimental Setup
物理试验台。我们在由 32 台服务器和 64 个 GPU 组成的异构集群上评估 Crius。具体来说，16台服务器配备了Intel Xeon Gold 5318Y CPU（256 GB内存）和2个Nvidia A40 GPU（48 GB内存）。其他 16 台服务器配备相同的 CPU，但配备 2 个 Nvidia A10 GPU（24 GB 内存）。具有 A40 GPU 的服务器使用 Nvidia Mellanox Infiniband ConnectX-5 [39] 互连，而 A10 GPU 服务器使用 Nvidia Mellanox Infiniband ConnectX-6 [40] 互连。

具有较高异质性的模拟集群。模拟集群拥有 1,280 个 GPU，分为四种类型：A100、A40、Ampere 架构的 A10 和 Volta 架构的 V100。详细规格如表1所示。

工作负载。我们使用表 2 中的三个大型模型作为工作负载。所选模型与最先进的工作相同[8]。我们使用 13,000 多个职位的两周追踪记录从生产费城追踪。此外，我们还使用Helios Venus追踪[47]和阿里巴巴PAI追踪[48]来评估Crius。虽然每个作业记录由作业 ID、提交时间和持续时间组成，但我们随机生成 GPU 数量和 GPU 类型，以使跟踪适应异构场景。 Philly 跟踪中的迭代次数是通过按全局批量大小进行缩放来计算的。在另外两个跟踪中，我们根据跟踪工作负载随机生成迭代量。

基线。我们将 Crius 与四个基线进行比较：

先来先服务（FCFS）：FCFS 广泛应用于集群调度程序（例如 Kubernetes [49]、Yarn [50]）。 

Gandiva：Gandiva [15] 利用特定领域的知识基于运行时分析来内省地完善调度决策。虽然它可以调整 GPU 配额和拓扑，但它忽略了 GPU 异构性。 

Gavel：Gavel [20] 是一种异构感知调度器，设计用于各种调度策略，包括吞吐量最大化。虽然考虑了GPU异构性，但不支持GPU数量的缩放。 

ElasticFlow：ElasticFlow [19] 是一种自适应感知调度程序，可以在同构集群中弹性扩展分布式深度学习作业。它提供了截止日期感知策略和面向吞吐量的策略。

需要指出的是，Crius 是第一个支持资源扩展和自适应并行联合使用的作品。为了公平比较，我们在基线的作业训练过程中启用 Alpa 的自适应并行性，但只允许它们使用数据并行性分析的数据来安排作业。